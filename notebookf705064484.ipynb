{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Credit Card Fraud Detection â€“ Single Kaggle Notebook\n# Concepts: Imbalanced datasets, anomaly detection, precision-recall metrics\n# Models: Logistic Regression, Random Forest, XGBoost, Isolation Forest baseline\n\n# =========================\n# 1) Imports & Setup\n# =========================\nimport os, sys, warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    roc_auc_score,\n    average_precision_score,\n    precision_recall_curve,\n    roc_curve,\n    classification_report,\n    confusion_matrix\n)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, IsolationForest\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline  # to use SMOTE in pipeline\n\nimport matplotlib.pyplot as plt\n\n# XGBoost\nfrom xgboost import XGBClassifier\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\n# =========================\n# 2) Load Data\n# =========================\n# Adjust if your dataset path is different\nDATA_PATH = \"/kaggle/input/creditcardfraud/creditcard.csv\"\nif not os.path.exists(DATA_PATH):\n    raise FileNotFoundError(\n        f\"Dataset not found at {DATA_PATH}. \"\n        \"In Kaggle, add the dataset 'Credit Card Fraud Detection' to the notebook, \"\n        \"or update DATA_PATH.\"\n    )\n\ndf = pd.read_csv(DATA_PATH)\nprint(\"Shape:\", df.shape)\nprint(df.head())\n\n# Basic sanity checks\nassert \"Class\" in df.columns, \"Expected target column 'Class' not found.\"\ntarget = \"Class\"\n\n# =========================\n# 3) Quick EDA: Class Imbalance\n# =========================\nclass_counts = df[target].value_counts().sort_index()\nfraud_ratio = class_counts[1] / class_counts.sum()\nprint(\"\\nClass distribution:\")\nprint(class_counts)\nprint(f\"Fraud ratio: {fraud_ratio:.6f} ({fraud_ratio*100:.4f}%)\")\n\n# Optional: show a bar chart\nplt.figure()\nclass_counts.plot(kind=\"bar\")\nplt.title(\"Class Distribution (0=Legit, 1=Fraud)\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# =========================\n# 4) Train/Validation Split\n# =========================\nX = df.drop(columns=[target])\ny = df[target].astype(int)\n\n# Identify numeric columns (these are all numeric in this dataset)\nnum_cols = X.columns.tolist()\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n)\n\nprint(f\"\\nTrain shape: {X_train.shape}, Valid shape: {X_valid.shape}\")\nprint(\"Train class balance:\", np.bincount(y_train))\nprint(\"Valid class balance:\", np.bincount(y_valid))\n\n# =========================\n# 5) Helpers: Metrics & Threshold Tuning\n# =========================\ndef evaluate_probs(y_true, y_prob, name=\"model\", plot_curves=True):\n    \"\"\"Compute ROC-AUC, PR-AUC (Average Precision), and optionally plot curves.\"\"\"\n    roc = roc_auc_score(y_true, y_prob)\n    pr_auc = average_precision_score(y_true, y_prob)\n\n    if plot_curves:\n        # Precision-Recall\n        precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n        plt.figure()\n        plt.plot(recall, precision)\n        plt.xlabel(\"Recall\")\n        plt.ylabel(\"Precision\")\n        plt.title(f\"Precision-Recall Curve: {name} (AP={pr_auc:.4f})\")\n        plt.show()\n\n        # ROC\n        fpr, tpr, _ = roc_curve(y_true, y_prob)\n        plt.figure()\n        plt.plot(fpr, tpr)\n        plt.plot([0,1],[0,1], linestyle=\"--\")\n        plt.xlabel(\"FPR\")\n        plt.ylabel(\"TPR\")\n        plt.title(f\"ROC Curve: {name} (AUC={roc:.4f})\")\n        plt.show()\n\n    return {\"model\": name, \"roc_auc\": roc, \"pr_auc\": pr_auc}\n\ndef tune_threshold_for_precision(y_true, y_prob, min_precision=0.90):\n    \"\"\"Return the highest threshold that achieves at least `min_precision` (default 0.90).\n       Fallback to threshold maximizing F1 if precision target not achievable.\"\"\"\n    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n    thresholds = np.r_[0, thresholds]  # align lengths (precision/recall start at threshold=0)\n\n    # Find thresholds where precision >= min_precision\n    mask = precision >= min_precision\n    viable_thresholds = thresholds[mask]\n\n    if len(viable_thresholds) > 0:\n        thr = viable_thresholds[-1]  # highest threshold achieving target precision\n        return thr, {\"precision\": precision[mask][-1], \"recall\": recall[mask][-1]}\n    else:\n        # Maximize F1 as fallback\n        f1s = (2 * precision * recall) / (precision + recall + 1e-12)\n        idx = np.nanargmax(f1s)\n        return thresholds[idx], {\"precision\": precision[idx], \"recall\": recall[idx]}\n\ndef print_confusion_and_report(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    print(\"Confusion Matrix:\\n\", cm)\n    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=4))\n\n# =========================\n# 6) Baseline Anomaly Detection (Isolation Forest)\n#    Trained on majority class only\n# =========================\n# IsolationForest scores: negative values anomalous; we convert to probabilities-ish via min-max scaling\niso = IsolationForest(\n    n_estimators=200,\n    contamination=fraud_ratio,  # approximate proportion of anomalies\n    random_state=RANDOM_STATE\n)\n\n# Fit only on legit transactions from training set\nX_train_legit = X_train[y_train == 0]\niso.fit(X_train_legit)\n\n# Score validation\niso_scores = -iso.decision_function(X_valid)  # higher => more anomalous\n# Normalize to [0,1] for comparability\niso_prob = (iso_scores - iso_scores.min()) / (iso_scores.max() - iso_scores.min() + 1e-9)\n\nres_table = []\nres_table.append(evaluate_probs(y_valid, iso_prob, name=\"IsolationForest\", plot_curves=True))\n\n# =========================\n# 7) Logistic Regression (with Standardization + SMOTE)\n# =========================\nlogreg_pipeline = ImbPipeline(steps=[\n    (\"scaler\", StandardScaler(with_mean=False)),  # with_mean=False for sparse-safety (though data is dense)\n    (\"smote\", SMOTE(random_state=RANDOM_STATE, sampling_strategy=\"auto\", k_neighbors=5)),\n    (\"clf\", LogisticRegression(max_iter=500, class_weight=None, solver=\"liblinear\", random_state=RANDOM_STATE))\n])\n\nlogreg_pipeline.fit(X_train, y_train)\nlogreg_prob = logreg_pipeline.predict_proba(X_valid)[:, 1]\nres_table.append(evaluate_probs(y_valid, logreg_prob, name=\"LogReg+SMOTE\", plot_curves=True))\n\n# =========================\n# 8) Random Forest (with SMOTE)\n# =========================\nrf_pipeline = ImbPipeline(steps=[\n    (\"smote\", SMOTE(random_state=RANDOM_STATE, sampling_strategy=\"auto\", k_neighbors=5)),\n    (\"clf\", RandomForestClassifier(\n        n_estimators=200,\n        max_depth=None,\n        n_jobs=-1,\n        random_state=RANDOM_STATE\n    ))\n])\n\nrf_pipeline.fit(X_train, y_train)\nrf_prob = rf_pipeline.predict_proba(X_valid)[:, 1]\nres_table.append(evaluate_probs(y_valid, rf_prob, name=\"RandomForest+SMOTE\", plot_curves=True))\n\n# =========================\n# 9) XGBoost (scale_pos_weight instead of SMOTE)\n#    scale_pos_weight ~ (neg / pos) helps skewed data\n# =========================\nneg, pos = np.bincount(y_train)\nscale_pos_weight = neg / max(pos, 1)\n\nxgb = XGBClassifier(\n    n_estimators=300,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    reg_lambda=1.0,\n    objective=\"binary:logistic\",\n    eval_metric=\"aucpr\",  # focus on PR\n    tree_method=\"hist\",\n    random_state=RANDOM_STATE,\n    scale_pos_weight=scale_pos_weight,\n    n_jobs=-1\n)\n\nxgb.fit(X_train, y_train)\nxgb_prob = xgb.predict_proba(X_valid)[:, 1]\nres_table.append(evaluate_probs(y_valid, xgb_prob, name=\"XGBoost(spw)\", plot_curves=True))\n\n# =========================\n# 10) Compare Models by PR-AUC (Average Precision)\n# =========================\nresults_df = pd.DataFrame(res_table).sort_values(\"pr_auc\", ascending=False)\nprint(\"\\nModel comparison (sorted by PR-AUC):\")\nprint(results_df)\n\n# =========\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T12:51:22.628068Z","iopub.execute_input":"2025-09-03T12:51:22.628345Z","iopub.status.idle":"2025-09-03T12:51:24.449051Z","shell.execute_reply.started":"2025-09-03T12:51:22.628324Z","shell.execute_reply":"2025-09-03T12:51:24.447080Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2285125798.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColumnTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mImbPipeline\u001b[0m  \u001b[0;31m# to use SMOTE in pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# process, as it may not be compiled yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     from . import (\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mcombine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mensemble\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/combine/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_enn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTEENN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_tomek\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTETomek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/combine/_smote_enn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseOverSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneToOneFeatureMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMETHODS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.utils._metadata_requests'"],"ename":"ModuleNotFoundError","evalue":"No module named 'sklearn.utils._metadata_requests'","output_type":"error"}],"execution_count":1}]}