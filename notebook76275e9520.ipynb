{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":169835,"sourceType":"datasetVersion","datasetId":74977}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Student Performance Prediction - End-to-End ML Pipeline\n# -------------------------------------------------------\n# Covers: Regression + Classification, Preprocessing, Model Comparison\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom xgboost import XGBRegressor, XGBClassifier\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n\n# ==============================\n# Load Dataset\n# ==============================\n# Kaggle dataset: https://www.kaggle.com/datasets/spscientist/students-performance-in-exams\ndf = pd.read_csv(\"/kaggle/input/students-performance-in-exams/StudentsPerformance.csv\")\n\nprint(\"Shape:\", df.shape)\nprint(df.head())\n\n# ==============================\n# Feature Engineering\n# ==============================\n# Convert categorical columns\ncategorical = df.select_dtypes(include=[\"object\"]).columns.tolist()\nnumerical = df.select_dtypes(exclude=[\"object\"]).columns.tolist()\n\n# Example: average score as regression target\ndf[\"average_score\"] = df[[\"math score\", \"reading score\", \"writing score\"]].mean(axis=1)\n\n# Example: classification target (pass/fail if avg >= 50)\ndf[\"passed\"] = (df[\"average_score\"] >= 50).astype(int)\n\n# ==============================\n# Preprocessing Pipelines\n# ==============================\nnum_transformer = Pipeline(steps=[\n    (\"scaler\", StandardScaler())\n])\n\ncat_transformer = Pipeline(steps=[\n    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", num_transformer, numerical),\n        (\"cat\", cat_transformer, categorical)\n    ]\n)\n\n# ==============================\n# Regression Models\n# ==============================\nX = df.drop(columns=[\"average_score\", \"passed\"])\ny_reg = df[\"average_score\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n\nreg_models = {\n    \"Linear Regression\": LinearRegression(),\n    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=200, random_state=42),\n    \"XGBoost Regressor\": XGBRegressor(n_estimators=300, learning_rate=0.05, random_state=42)\n}\n\nprint(\"\\n===== Regression Models =====\")\nfor name, model in reg_models.items():\n    reg_pipe = Pipeline(steps=[(\"preprocessor\", preprocessor),\n                               (\"regressor\", model)])\n    reg_pipe.fit(X_train, y_train)\n    y_pred = reg_pipe.predict(X_test)\n    \n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    r2 = r2_score(y_test, y_pred)\n    \n    print(f\"{name}: RMSE={rmse:.4f}, R²={r2:.4f}\")\n\n# ==============================\n# Classification Models\n# ==============================\nX = df.drop(columns=[\"average_score\", \"passed\"])\ny_clf = df[\"passed\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_clf, test_size=0.2, random_state=42, stratify=y_clf)\n\nclf_models = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n    \"Random Forest Classifier\": RandomForestClassifier(n_estimators=200, random_state=42),\n    \"XGBoost Classifier\": XGBClassifier(n_estimators=300, learning_rate=0.05, use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n}\n\nprint(\"\\n===== Classification Models =====\")\nfor name, model in clf_models.items():\n    clf_pipe = Pipeline(steps=[(\"preprocessor\", preprocessor),\n                               (\"classifier\", model)])\n    clf_pipe.fit(X_train, y_train)\n    y_pred = clf_pipe.predict(X_test)\n    y_prob = clf_pipe.predict_proba(X_test)[:, 1]\n    \n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred)\n    rec = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    roc = roc_auc_score(y_test, y_prob)\n    \n    print(f\"{name}: Accuracy={acc:.4f}, Precision={prec:.4f}, Recall={rec:.4f}, F1={f1:.4f}, ROC-AUC={roc:.4f}\")\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T09:56:21.058123Z","iopub.execute_input":"2025-08-30T09:56:21.058584Z","iopub.status.idle":"2025-08-30T09:56:23.013298Z","shell.execute_reply.started":"2025-08-30T09:56:21.058550Z","shell.execute_reply":"2025-08-30T09:56:23.011865Z"}},"outputs":[{"name":"stdout","text":"Shape: (1000, 8)\n   gender race/ethnicity parental level of education         lunch  \\\n0  female        group B           bachelor's degree      standard   \n1  female        group C                some college      standard   \n2  female        group B             master's degree      standard   \n3    male        group A          associate's degree  free/reduced   \n4    male        group C                some college      standard   \n\n  test preparation course  math score  reading score  writing score  \n0                    none          72             72             74  \n1               completed          69             90             88  \n2                    none          90             95             93  \n3                    none          47             57             44  \n4                    none          76             78             75  \n\n===== Regression Models =====\nLinear Regression: RMSE=0.0000, R²=1.0000\nRandom Forest Regressor: RMSE=1.1100, R²=0.9943\nXGBoost Regressor: RMSE=0.8961, R²=0.9963\n\n===== Classification Models =====\nLogistic Regression: Accuracy=0.9800, Precision=0.9834, Recall=0.9944, F1=0.9889, ROC-AUC=0.9984\nConfusion Matrix:\n [[ 18   3]\n [  1 178]]\nRandom Forest Classifier: Accuracy=0.9850, Precision=0.9889, Recall=0.9944, F1=0.9916, ROC-AUC=0.9995\nConfusion Matrix:\n [[ 19   2]\n [  1 178]]\nXGBoost Classifier: Accuracy=0.9950, Precision=1.0000, Recall=0.9944, F1=0.9972, ROC-AUC=1.0000\nConfusion Matrix:\n [[ 21   0]\n [  1 178]]\n","output_type":"stream"}],"execution_count":2}]}